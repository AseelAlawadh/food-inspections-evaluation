---
title: "Forecasting restaurants with critical violations in Chicago"
author: "City of Chicago"
date: "December 24, 2014"
output:
  pdf_document:
    number_sections: yes
  html_document:
    css: assets/journal-chicago/css/journal-chicago.css
    number_sections: yes
  word_document: default
---


```{r initialize, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
##==============================================================================
## INITIALIZE
##==============================================================================
## Remove all objects; perform garbage collection
rm(list=ls())
gc(reset=TRUE)
## Check for dependencies
if(!"geneorama" %in% rownames(installed.packages())){
    if(!"devtools" %in% rownames(installed.packages())){
        install.packages('devtools')}
    devtools::install_github('geneorama/geneorama')}
## Load libraries
geneorama::detach_nonstandard_packages()
geneorama::loadinstall_libraries(c("data.table", "ggplot2", "knitr", "glmnet"))
geneorama::set_project_dir("food-inspections-evaluation")
geneorama::sourceDir("CODE/functions/")
library(RSocrata)
library(scales)

## Globally turn off "fixing" indenting for code chunks
opts_chunk$set(tidy = FALSE)

## Set figure output
opts_knit$set(fig.path = "assets/figure")

## Navigate to the top level directory 
geneorama::set_project_dir("food-inspections-evaluation")
```
```{r define_global_variables, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
##==============================================================================
## DEFINE GLOBAL VARIABLES / MANUAL CODE
##==============================================================================
## Select data version
DataDir <- "DATA/20141110"
```
```{r prepare_for_model, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
##==============================================================================
## LOAD CACHED RDS FILES AND MODIFY DATA FOR MODEL
##==============================================================================
dat <- readRDS("DATA/dat_model.Rds")
## Only keep "Retail Food Establishment"
dat <- dat[LICENSE_DESCRIPTION == "Retail Food Establishment"]
## Remove License Description
dat[ , LICENSE_DESCRIPTION := NULL]
dat <- na.omit(dat)
## Add criticalFound variable to dat:
dat[ , criticalFound := pmin(1, criticalCount)]
## Set the key for dat
setkey(dat, Inspection_ID)
## Match time period of original results
# dat <- dat[Inspection_Date < "2013-09-01" | Inspection_Date > "2014-07-01"]
```
```{r create_model_data, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
##==============================================================================
## CREATE MODEL DATA
##==============================================================================
xmat <- dat[ , list(criticalFound,
                    Inspector = Inspector_Assigned,
                    pastSerious = pmin(pastSerious, 1),
                    ageAtInspection = ifelse(ageAtInspection > 4, 1L, 0L),
                    pastCritical = pmin(pastCritical, 1),
                    consumption_on_premises_incidental_activity,
                    tobacco_retail_over_counter,
                    temperatureMax,
                    heat_burglary = pmin(heat_burglary, 70),
                    heat_sanitation = pmin(heat_sanitation, 70),
                    heat_garbage = pmin(heat_garbage, 50),
                    # risk = as.factor(Risk),
                    # facility_type = as.factor(Facility_Type),
                    timeSinceLast),
            keyby = Inspection_ID]
mm <- model.matrix(criticalFound ~ . -1, data=xmat[ , -1, with=F])
mm <- as.data.table(mm)
```
```{r create_test-train_partitions, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
##==============================================================================
## CREATE TEST / TRAIN PARTITIONS
##==============================================================================
iiTrain <- dat[ , which(Inspection_Date < "2014-07-01")]
iiTest <- dat[ , which(Inspection_Date > "2014-07-01")]
```
```{r glmnet_model, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
##==============================================================================
## GLMNET MODEL
##==============================================================================
# fit ridge regression, alpha = 0, only inspector coefficients penalized
pen <- ifelse(grepl("^Inspector", colnames(mm)), 1, 0)
model <- glmnet(x = as.matrix(mm[iiTrain]),
                y = xmat[iiTrain,  criticalFound],
                family = "binomial", alpha = 0, penalty.factor = pen)
w.lam <- 100
lam <- model$lambda[w.lam]
coef <- model$beta[,w.lam]
coefInsp <- coef[grepl("^Inspector",names(coef))]
coefInsp <- coefInsp[order(-coefInsp)]
coefNonInsp <- coef[!grepl("^Inspector",names(coef))]

## ATTACH PREDICTIONS TO DAT
dat$glm_pred <- predict(model, newx=as.matrix(mm), s=lam, type="response")[,1]
```
```{r prepare_data_for_evaluation_summary, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
# ## Subset test period
datTest <- dat[iiTest]

## Identify the ACTUAL first and second periods
datTest[ , period := ifelse(Inspection_Date < median(Inspection_Date), 1, 2)]
datTest[, .N, keyby=list(period)]
## Identify top half of scores (which *would have* been the first period)
datTest[ , period_modeled := ifelse(glm_pred > median(glm_pred), 1, 2)]
datTest[, .N, keyby=list(period_modeled)]

## Total of inspections / critical found for ACTUAL data
datTest[, list(.N, Violations = sum(criticalFound)), keyby=list(period)]

## Total of inspections / critical found for MODELED scenario
datTest[, list(.N, Violations = sum(criticalFound)), keyby=list(period_modeled)]

## Another way to say "how many would you have found  in the first period"
datTest[period == 1, sum(criticalFound)]
datTest[period_modeled == 1, sum(criticalFound)]
```
```{r prepare_summary_of_results, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
##==============================================================================
## CALCULATE DATA FOR summary_of_results_table
##==============================================================================

## Total of inspections / critical found for ACTUAL data
baseline <- datTest[i = TRUE, 
                    j = list(.N, 
                             Violations = sum(criticalFound)), 
                    keyby = list(period)]

## Total of inspections / critical found for MODELED scenario
data_driven <- datTest[i = TRUE,
                       j = list(.N,
                                Violations = sum(criticalFound)),
                       keyby = list(period_modeled)]

summary_of_eval_results_table <- data.frame(Period = baseline$period, 
                                            `Number of Inspections` = baseline$N, 
                                            `Baseline Violations` = baseline$Violations, 
                                            `Data-driven Violations` = data_driven$Violations,
                                            check.names = FALSE)
summary_of_eval_results_table
```

> The Chicago Department of Public Health (CDPH) inspects more than 15,000 restaurants with fewer than three dozen inspectors over the course of the year. This paper develops a predictive model design to identify the likelihood any restaurant that contains critical violations. Since CDPH is obligated to inspect every food establishment, the goal of the model is to identify the riskiest restaurants earlier, thereby reducing the length of exposure of risky restaurants to patrons. In testing, the predictive model was able to identify 23 percent more violations than current operations.

# Introduction

The [Chicago Department of Public Health](http://www.cityofchicago.org/city/en/depts/cdph.html) inspects more than 15,000 restaurants with fewer than three dozen inspectors. City of Chicago ordinance requires that most of these establishments must be inspected at least once a year. The task to inspect each restaurant, while also performing other inspections, is completed over the year. 

CDPH conducts three different types of inspections. First, CDPH conducts license inspections for any new establishment with a food license prior to the establishment opening. Each establishment must pass this initial inspection before it is allowed to serve food to patrons. Second, CDPH will conduct canvas inspections; periodic inspections to check the quality of sanitary conditions. The number and frequency of inspections is driven by the type of facility and how it prepares food, inspecting the riskiest restaurants at least two times a year.

License inspections are coordinated with [Business Affairs and Consumer Protection (BACP)](http://www.cityofchicago.org/city/en/depts/bacp.html), who grants food establishment licenses to new establishments. The quantity and location of these inspections is driven by license applications, thereby, dependent on the economy, entrepreneurship, and other outside factors beyond the control of the City. These inspections can be characterized as routine, but not a guarantee to pass. Establishments fail these initial inspections because they have not yet finished setting-up equipment, such as turning-on a refrigerator, or have not finished construction. Under these circumstances, CDPH will re-inspect those establishments to ensure those conditions are passed before they are allowed to open.

Complaints are registered from residents, alderman, and referrals from hospitals. Often, these requests are driven through the City of Chicago's 311 system, which can be submitted through residents calling 311 or submitting a request through an online form. Individuals are asked to submit where they believe they contracted food poisoning, the address of the establishment, describe the symptoms and what was eaten, and when it happened. CDPH reviews the materials and may initiate a food inpsection if it does seem the illness and restaurant can be linked together.

Uniquely, CDPH also encourages submissions through the [Foodborne Chicago](http://www.cdc.gov/mmwr/preview/mmwrhtml/mm6332a1.htm) program. [Machine learning algorithms](https://github.com/smartchicago/foodborne) scans Twitter for individuals complaining or indicating potential food poisioning cases. These tweets are identified and a human will contact the user, providing a link and information on how they can [report their complaint](https://www.foodbornechicago.org/) to CDPH. In a nine-month span, 133 food inspections were instigated from this program, where 20 percent (27 instances) of those inspections resulted in critical violations (Harris et al. 2014)

The Foodborne program and 311 system has assisted CDPH in targeting and identifying complaint-driven requests. Yet, the department has a sizeable task to complete canvas inspections. Canvas inspections occur throughout the year and are somewhat random inspections of various restaurants. The process is key to checking and enforcing consistent food safety practices throughout the city. Identifying critical issues at restaurants help rectify those issues and reduce exposure to patrons. 

The work is organized by CDPH-defined "risk levels", which are divided into three categories: risk 1 (highest), risk 2, and risk 3. The risk levels are determined by food handling practices required for each establishment. Restaurants and other establishments that directly handle ingredients and prepare food, such as needs to cool or heat food, are generally categorized as risk 1. The lowest risk generally consists of prepackages and non-perishable food. Risk level also drives frequency of inspection. Risk 1 facilities are inspected more frequently, with a target of at least twice a year; risk 2 are inspected at least once a year; and risk 3 is inspected once every other year.

Risk levels do help prioritize inspections by focusing on establishments with the highest likelihood of spreading food born illnesses through categorization of food handling practices. However, `r percent(table(dat$Risk)[1]/sum(table(dat$Risk)))` of those establishments in our dataset are categorized as risk 1. The high proportion of risk 1 establishments means there is still a substantial queue to be inspected. Yet, the work is certainly achievable. Assuming 32 inspectors, each inspector would need to complete `r format(round(table(dat$Risk)[1]/32/230, 0), nsmall=0)` canvass inspections each working day---in addition to complaint-driven and new license inspections.

There are 42 different possible violations that can be cited by CDPH. Often, these violations are classified into three categories: critical, serious, and minor violations. Critical violations consist of 9 different violations that are most likely to create conditions for food born illnesses, such as failure to heat food to proper temperatures or to keep items properly fridgerated at the proper temperatures. Conversely, minor violations can be as simple as leaving a rag in the sink. Restaurants can fail their inspections with as little as one critical violation. Several serious and minor violations during an inspection can also lead to a failed outcome.

The clear priority of CDPH food inspection team is to prevent foodborne illnesses, which are most likely to stem from critical violations. With a XX risk 1 establishments to inspect throughout the year, the question also extends to which establishments should be inspected first. Fortunately, the city dollects data on multiple facets of food inspections, including the outcomes of inspections, information about businesses and their activities, and events around each food establishment, such as 311 complaints, crime, and even weather. Section 2 describes data that has been collected by the research team for this project.

Section 3 describes how these data sources can be combined to show the relationship between the characteristics of a food seller with critical violations. We formulates a model to derive the likelihood for each establishment to have critical violations. This model can be used to prioritize which restaurants should be inspected first. By targeting the highest-probability restaurants, CDPH can minimize the amount of exposure restaurant patrons have to the unsanitary conditions that are most likely to lead to food born illnesses. This paper will focus on prioritizing inspections of risk 1 and risk 2 restaurants, since these must be inspected at least once a year.

After developing the statistical model to predict critical violations, the research team evaluates whether the model could optimize food inspection processes (section 4). Namely, the model is used to determine how much faster the food inspection team can discover critical violations. The team uses a simulation to compare real-life results to an alternate, data-driven arrangement.

Section 5 summarizes the results of the paper. Ultimately, we find that XX percent increase in finding critical violations at food sellers. Beginning in 2015, CDPH will begin to use this analytical model to prioritize canvas inspections. Each risk 1 and risk 2 restaurant will still undergo inspections; however, these restaurants with the highest likelihood of the most serious issues will be prioritized.

Finally, it is worth nothing that this research is an open source project. The source code of the statistical model is available on the City of Chicago [food inspection project page](http://github.com/Chicago/food-inspections-evaluation). The statistical modeling was completed using the R statistical software, with all the necessary data available online. This paper was generated using knitr, which allows others to view the underlying calculations to generate the summaries, tables, and diagrams in this document. The document is available in the same aforementioned repository.

# Data

The City of Chicago publishes over 600 datasets on the [open data portal](https://data.cityofchicago.org), including the results of food inspections from 2011 to present. The [food inspection dataset](https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5) includes the name of the establishment, address, risk level, inspection date, results, and a detailed list of violations found during the inspection.

At this time, the food inspection database is not hosted by the City of Chicago, instead, a file of all food inspections are sent to the City of Chicago open data team on a daily basis, which is automatically uploaded to the portal every morning. Thus, the rawest form of data available to the research team was the data available on the open data portal.

In addition, the City also publishes other relevant data on the portal, including: business licenses from 2011 to present, detailed crime data from 2011 to present, and various 311 data, including garbage and sanitation complaints.

Food inspection history is combined with business license data published by BACP. Any food seller must be licensed by the City, but must also obtain licenses for other activities, such as cigarette sales and liqour licenses. The license data provides other information about the business, including when cetain licenses were first obtained--an approximation for the age of business.

The location of the businesses are used to calculate nearby activity. Several variables were explored, but after conduting some data mining, we settled on burglaries, sanitation code complaints, and garbage cart requests. The density of each activity was calculated and stored.

Weather data was obtained from [forecast.io](www.forecast.io). The data contains a significant wealth of information on not only highs, lows, and percipitation, but also on granular weather forecasts for any latitude and longitude. After several conversations with CDPH staff, we focused on the relationship of temperatures and inspections. High temperatures can lead to issues to cooling food within a food establishment, which results in a critical violation. Some empirical testing of that hypothesis helped support its inclusion.

[Table of variables]

# Model Development

The principle question is whether we can reasonably determine the probability that a restaurant inspection will yield at least one critical violation. That is, the focus will be whether or not any critical violation is found---a binary response. We use a glmnet model to estimate the impact of 

While the following form can be expressed a number of ways, the logistic form is commonly expressed as the "log-odds transformation". 

$$
\begin{aligned}
\log = \frac{\text{Pr}(V=1|X=x)}{\text{Pr}(V=0|X=x)} = \beta_0 + \beta^T x
\end{aligned}
$$

Thus, the objective function is to minimize 

$$
\begin{aligned}
\min_{(\beta_0, \beta) \in \mathbb{R}^{p+1}} -\left[\frac{1}{N} \sum_{i=1}^N y_i \cdot (\beta_0 + x_i^T \beta) - \log (1+e^{(\beta_0+x_i^T \beta)})\right] + \lambda \big[ (1-\alpha)||\beta||_2^2/2 + \alpha||\beta||_1\big]
\end{aligned}
$$

A review of the methods to find this solution is provided by Friedman, Trevor, and Tibshirani (2010), whose glmnet library for R was used to provide estimates.

## Significant Variables 

The regression analysis shows the inspectors have a significant impact of the rate of finding critical violations. To provide anonymity, inspectors were grouped by similar performance estimated by individual regression coefficients. 

Whether the restaurant had a critical violation in the past was a positive perdictor of future critical violations.  Likewise, historical serious violations was also a predictor of future performance. In effect, past performance perdicted future outcomes, with those with critical violations more likely to repeat those violations than even those with, at most, serious violations.

The elapsed time since the last violation also increased the likelihood that inspectors found critical violations. However, restaurants probability of having critical violations fell over the lifespan of the restaurant. As restaurants grow older, they are less likely to have critical violations while long time-periods between inspections increased the likelihood.

Characteristics besides the restaurant itself were also indicators of future performance. Trends in weather, nearby reports of burglary, and complaints about sanitation and garbage seems to explain increases. An increase in the moving three-day average high temperature was associated with more critical violations. In conversations with inspection managers, researchers understood this to be associated with potential mechanical failures---driven by the heat---of equipment that maintained food temperature, a main source of critical violations.

Sanitation code complaints are one of the top complaints registered with the City of Chicago through its 311 system (including web and text reports). Sanitation code complaints include:




```{r table_of_coefficients, echo=FALSE}
knitr::kable(model$beta[,w.lam], digits=2, caption="Table of Coefficients")
```

# Evaluation

After formulating the analytical model, the the principal question for researchers turned to whether this analytical model provides more efficiency for the food inspection team. CDPH operational procedures requires the department to inspect every risk 1 and risk 2 restaurant. Therefore, the operational goal is to allow inspectors to discover critical violations earlier than their current operations.

The researchers applied a protocol to discover whether the analytical model could accelerate the rate of finding critical violations. This objective was slightly different than what the researchers initially sought to find: trying to find an increase in restaurants found with critical violations.

## Evaluation Design

The analytical model was trained on data from January 2011 through January 2014, which results were described in the previous sections. 

[INSERT FIT CODE]

The researchers waited until CDPH completed food inspections in September and October 2014. This timeframe ensured significant time passed between the test period (January 2011 through January 2014) and the evaluation period. It's likely any temporal correlation would subside between the test and evaluation period. CDPH was not aware this timeframe would be used for an evaluation in order to prevent against a Hawthorne Effect or other bias. Again, to reduce any potential to bias within reason, senior management at CDPH was aware of on-going research, but sanitarians were not informed of the research. Finally, several months passed between model development and the evaluation period, reducing a perception of the evaluation period.

The evaluation period lasted two months, from ```r #dat[iiTest, range(Inspection_Date)][1]``` to ```r #dat[iiTest, range(Inspection_Date)][2]``` and calculate the percentage of inspections that result in critical violations in the first half of the inspections during this period. The number of violations found during this period can be considered as status quo or current mode of operation. It serves as a baseline to capture performance levels of sanitarians, namely, the proportion and rate of critical violations that are found.

Meanwhile, we calculate the point predictions for each establishment using the training data from 2011 through 2014. The training data does not include the evaluation period so not to provide additional feedback from the evaluation period. We sort the establishments that were inspected during the evaluation in descending order of predicted values, placing the highest risk restaurants at the top of the list.

We calculate the percentage of those restaurants that would be inspected in the first half if the predictive model was used. The difference between the percentage of establishments found with critical violations during this period reflects the relative gain or loss of efficiency. Finding a greater percentage of critical violations with the predictive model indicates results can be found earlier. A similiar or reduced amount indicates the predictive model provides no benefit or is less efficient, respectively.

Note that this experimental design is assumed to yield the name number of restaurants found with critical violations. Indeed, under the premise that CDPH will inspect all restaurants, researchers will presume the number of violations will remain relatively the same. The objective of the model is to find critical violations earlier throughout the year.

## Results

CDPH completed ```r dat[iiTest, .N]``` inspections between ```r #dat[iiTest, range(Inspection_Date)][1]``` and ```r #dat[iiTest, range(Inspection_Date)][2]```. During this time, CDPH found ```r dat[iiTest, sum(criticalFound)] ``` violations, ```r dat[iiTest, 100*(sum(criticalFound)/.N)]```percent of all inspections. The rate of violations is consistent with the historical average of approximately 15 percent. While the rate of violations is slightly higher, it is close enough where we do not suspect this period is abnormal, thus, a valid comparison for our evaluation.
```{r prepare_data, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
# ## Subset test period
datTest <- dat[iiTest]

## Identify the ACTUAL first and second periods
datTest[ , period := ifelse(Inspection_Date < median(Inspection_Date), 1, 2)]
datTest[, .N, keyby=list(period)]
## Identify top half of scores (which *would have* been the first period)
datTest[ , period_modeled := ifelse(glm_pred > median(glm_pred), 1, 2)]
datTest[, .N, keyby=list(period_modeled)]

## Total of inspections / critical found for ACTUAL data
datTest[, list(.N, Violations = sum(criticalFound)), keyby=list(period)]

## Total of inspections / critical found for MODELED scenario
datTest[, list(.N, Violations = sum(criticalFound)), keyby=list(period_modeled)]

## Another way to say "how many would you have found  in the first period"
datTest[period == 1, sum(criticalFound)]
datTest[period_modeled == 1, sum(criticalFound)]
```
The research was split in two periods: period 1 representing the first half of inspections and period 2 representing the latter half. The team could not simply divide violations in half to determine the two periods. The midway point, the 599th inspection, took place sometime during the day. The research team would not be able to determine which precise inspection was the 599th, only that is took place during the course of the day along with other inspections. The first period of inspections consisted of ```r summary_of_eval_results_table$"Number of Inspections"[1]``` inspections, ```r 100 * (summary_of_eval_results_table$"Number of Inspections"[1]/(summary_of_eval_results_table$"Number of Inspections"[1] + summary_of_eval_results_table$"Number of Inspections"[2]))``` percent of all inspections, were considered for the first period while ```r summary_of_eval_results_table$"Number of Inspections"[2]``` inspections were included in the second.

Thus, we focus on comparing the percentage of violations found in the first period to the second period.

```{r summary_of_eval_results, echo=FALSE, warning=FALSE}
kable(summary_of_eval_results_table, digits=0, caption="Summary of Findings")
```

Below shows the Gini curve for the modelled inspection period.
```{r}
## show gini performance of inspector model on test data set
dat[iiTest, gini(glm_pred, criticalFound, plot=TRUE)]
```




## Basic model performance

```{r, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
## show gini performance of inspector model on test data set
dat[iiTest, gini(glm_pred, criticalFound, plot=TRUE)]
```

# Summary






## Confustion matrix for one cutoff (.25)

```{r, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
## Calculate confusion matrix values for evaluation
calculate_confusion_values(actual = dat[iiTest, criticalFound],
                           expected = dat[iiTest, glm_pred], 
                           r = .25)
```



```{r, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
geneorama::loadinstall_libraries(c("reshape2", "MASS", "splines"))
##------------------------------------------------------------------------------
## ANALYZE DATA
##------------------------------------------------------------------------------
str(dat)

dat[ , .N, list(Results)]
dat[ , .N, list(Inspection_Type)]

## Annual summary of passes
PassYearly <- dat[ , .N, list(Results = Results, 
                              Year = round(Inspection_Date, "year"))]
PassYearly
dcast.data.table(PassYearly, Year ~ Results, value.var="N")

## Monthly summary of passes
PassMonthly <- dat[i = Inspection_Type == "Canvass" , 
                   j = list(N = as.numeric(.N)), 
                   by = list(Results = Results, 
                             Date = round(Inspection_Date, "month"))]
PassMonthly
dcast.data.table(data = PassMonthly, formula = Date ~ Results,
                 value.var = "N", fill = 0)

##------------------------------------------------------------------------------
## DAILY SUMMARY
##------------------------------------------------------------------------------
canvas_summary <- dat[Inspection_Type == "Canvass" & Results == "Fail",
                      list(N_fail = .N),
                      keyby = Inspection_Date][
                          dat[Inspection_Type == "Canvass",
                              list(N_total=.N),
                              keyby = Inspection_Date]]
setcolorder(canvas_summary, c('Inspection_Date', 'N_total', 'N_fail'))
canvas_summary[ , Pct_fail := N_fail / N_total]
geneorama::convert_datatable_IntNum(canvas_summary)
canvas_summary


ggplot(melt(canvas_summary, id.vars = "Inspection_Date"))+
    aes(Inspection_Date, value, colour=variable) +    
    geom_line() + facet_wrap(~variable, scales="free") + 
    stat_smooth(method = rlm, formula= y ~ ns(x,9), colour="black") +
    stat_smooth(method = lm, formula= y ~ ns(x,9), colour="blue")

##------------------------------------------------------------------------------
## WEEKLY SUMMARY
##------------------------------------------------------------------------------
canvas_summary_w <- dat[i = Inspection_Type == "Canvass" & Results == "Fail",
                        j = list(N_fail=.N),
                        keyby = list(week=geneorama::round_weeks(Inspection_Date))][
                            dat[i = Inspection_Type == "Canvass",
                                j = list(N_total=.N),
                                keyby = list(week=geneorama::round_weeks(Inspection_Date))]]
setcolorder(canvas_summary_w, c('week', 'N_total', 'N_fail'))
canvas_summary_w[,Pct_fail:=N_fail/N_total]
geneorama::convert_datatable_IntNum(canvas_summary_w)
canvas_summary_w
ggplot(melt(canvas_summary_w, id.vars = "week"))+
    aes(week, value, colour=variable) +    
    geom_line() + facet_wrap(~variable, scales="free") + 
    stat_smooth(method = rlm, formula= y ~ ns(x,9), colour="black") +
    stat_smooth(method = lm, formula= y ~ ns(x,9), colour="blue")
canvas_summary_w[,sum(N_total)]

```










